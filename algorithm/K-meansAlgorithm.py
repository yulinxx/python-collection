# K-means聚类算法原理及python实现_python kmeans-CSDN博客   https://blog.csdn.net/qq_312/article/details/745
# "聚类算法"试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。

# 实现kmeans算法的主要四点：
# （1）簇个数 k 的选择
# （2）各个样本点到“簇中心”的距离
# （3）根据新划分的簇，更新“簇中心”
# （4）重复上述2、3过程，直至"簇中心"没有移动
# 优缺点：

# 优点：容易实现
# 缺点：可能收敛到局部最小值，在大规模数据上收敛较慢


import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_blobs

# 若不用 create_data_set 函数创建数据的话，则可使用下列数组进行测试
date = [[-0.623, 9.188], [0.54, 5.305], [-2.300, 7.546], 
        [4.225, 2.824], [-3.181, 9.445], [-8.684, 7.548], [1.065, 3.927], [-6.435, -7.058], [-8.704, 7.105], 
        [2.716, 3.163], [3.685, 2.626], [-2.843, 7.421], [4.997, 2.376], [-3.026, 8.553], [1.043, 3.014], 
        [-3.784, 7.651], [4.743, 1.327], [4.283, 2.569], [0.997, 3.003], [-2.077, 7.203], [-1.296, 9.095], 
        [5.915, 3.679], [-2.195, 8.702], [1.316, 3.117], [2.794, 3.645], [-2.716, 7.523], [-6.324, -6.308], 
        [1.658, 3.108], [3.078, 2.184], [-7.702, -6.034], [-5.773, 8.279], [3.075, 3.162], [1.234, 3.835],
        [-7.487, 7.562], [-8.315, -5.117], [3.963, 0.736], [4.525, 2.424], [2.859, 3.074], [-8.629, 8.164], 
        [3.134, 2.956], [-4.914, 8.105], [5.761, 2.941], [-6.042, -6.661], [-6.023, 9.907], [2.755, 4.829],
        [-8.777, 6.278], [-2.016, 10.328], [-6.251, -7.707], [1.064, 4.761], [-4.004, -8.449], [-9.553, 7.547], 
        [2.765, 6.975], [-7.782, -7.894], [-7.726, 7.325], [-3.938, 8.785], [-10.785, 5.603], [3.874, 2.036],
        [-3.384, 8.555], [-10.562, 8.228], [-7.853, -6.137], [-2.888, 7.587], [-9.596, 7.926], [2.574, 3.165],
        [2.606, 6.305], [-6.231, -6.335], [5.235, 2.304], [-8.594, -8.502], [-6.318, -7.269], [-6.266, -5.394], 
        [-7.482, -6.922], [3.298, 3.714], [-7.972, -8.678], [-10.165, 7.085], [2.417, 4.755], [-9.084, 8.058], 
        [-2.743, 9.212], [-9.786, 6.814], [-9.193, 6.855], [2.275, 3.836], [-3.147, 10.726], [-5.918, -8.796], 
        [-3.961, 7.303], [-2.116, 8.197], [2.845, 3.885], [-2.403, 8.505], [-7.593, -6.311], [-10.888, 8.116], 
        [5.269, 3.331], [3.453, 2.617], [3.983, 4.122], [2.868, 3.068], [2.496, 2.488], [-5.494, 9.545], 
        [2.357, 5.859], [-3.789, 9.677], [-2.097, 9.121], [3.205, 3.573], [-7.184, 7.812], [3.194, 0.557], 
        [1.166, 4.323], [-10.378, 6.931], [-10.863, 7.742], [5.378, 3.347], [-5.195, -6.338], [-1.651, 7.041], 
        [-2.863, 9.068], [-4.783, 8.735], [6.161, 0.394], [-2.955, 8.005], [3.107, 2.356], [-8.885, 9.459], 
        [2.015, 4.975], [4.668, 3.098], [5.771, 0.768], [-1.676, 9.006], [-3.933, 9.187], [-1.561, 7.299], [-5.174, -6.991], 
        [-10.632, 8.213], [-9.313, 5.029], [-9.368, 7.209], [-5.519, -7.085], [-7.968, 8.391], [-6.338, 6.515], 
        [-1.798, 11.083], [-5.025, -6.011], [1.591, 2.452], [-0.208, 3.046], [-10.584, 7.926], [-1.992, 9.452], [5.615, 0.035], 
        [-9.655, 5.118], [3.103, 1.609], [-2.236, 11.608], [-2.262, 9.069], [-3.738, 9.595], [-2.376, 10.237], [3.325, 1.835], 
        [-5.635, -8.579], [1.026, 2.492], [-3.644, 8.962], [-8.111, 7.525], [-2.124, 8.065], [3.578, 1.937], [3.404, 5.359], 
        [-2.478, 7.429], [2.146, 4.772], [-5.648, -7.728], [3.965, 2.707], [-7.342, -7.916], [4.008, 3.049], [-6.232, -7.984], 
        [-6.307, -7.402], [4.783, 1.364], [-1.961, 8.787], [1.329, 4.068], [-7.545, -7.779], [-8.862, 8.051], [-2.893, 8.175], 
        [4.145, 1.702], [-8.718, 6.236], [-8.642, 6.495], [0.013, 4.291], [4.101, 0.089], [-2.579, 8.454], [-7.332, 7.055], 
        [1.185, 2.573], [2.665, 1.237], [4.384, 2.663], [2.383, 3.445], [-6.959, -7.555], [-2.726, 9.935], [4.978, 2.708], 
        [4.231, 3.603], [-1.033, 8.582], [3.864, 1.563], [-10.653, 9.057], [4.305, 0.744], [-6.778, -7.742], [-6.128, -6.005], 
        [-10.816, 6.942], [-7.996, 6.363], [5.075, 1.494], [-8.283, -7.198], [-6.185, -7.778], [2.886, 6.348], [-3.528, 9.993], 
        [-9.704, 6.752], [-7.254, -6.855], [5.304, 2.653], [2.378, 3.416], [-6.552, -6.033], [-2.264, 7.865], [-7.443, -8.795], 
        [-5.154, -8.386], [-8.585, 8.008], [-9.538, 6.023], [-6.455, -7.921], [2.584, 5.935], [4.915, 1.148], [-7.235, 6.999], 
        [-6.366, -7.085], [2.494, 2.598], [5.505, 1.215], [-8.569, 6.329], [-2.907, 8.033], [-3.776, 8.614], [-6.547, -7.076], 
        [-6.223, -7.601], [-2.084, 10.345], [-6.925, -7.374], [-6.621, 7.315], [0.344, 4.148], [-7.743, -4.486], [-2.336, 10.429], 
        [3.687, 2.595], [-7.225, 5.589], [-6.535, 6.653], [-4.637, -5.996], [1.588, 2.639], [-7.718, 5.115], [-8.182, 8.942], 
        [2.676, 4.857], [-9.845, 6.359], [1.822, 3.288], [5.999, 2.182], [-9.537, 6.308], [2.707, 4.245], [-9.709, 7.911], 
        [2.817, 6.039], [-8.163, 6.348], [4.145, 4.351], [5.559, 2.534], [-6.008, -6.482], [1.487, 3.126], [3.296, 1.231], 
        [4.121, 2.556], [4.231, 1.424], [2.564, 3.358], [-2.593, 9.286], [-9.568, 8.665], [6.427, 2.743], [-6.828, -7.315], 
        [-0.366, 3.432], [-5.846, -7.777], [-8.244, 7.224], [2.585, 5.809], [2.275, 5.192], [-6.098, -5.369], [-6.115, -6.667], 
        [3.893, 2.917], [0.193, 3.301], [-8.826, 7.445], [-6.853, -6.152], [-9.254, 7.202], [3.616, 3.776], [-7.491, -5.565], 
        [2.726, 3.646], [-6.158, -7.783], [2.025, 3.871], [-10.342, -7.236], [-1.397, 9.292], [2.421, 3.414], [6.858, 2.195], 
        [5.646, 1.591], [-3.389, 8.108], [3.833, 4.568], [3.965, 2.137], [-5.941, -6.187], [3.401, 1.833], [-8.712, 6.834], 
        [-6.022, -6.549], [-6.185, -6.165], [-2.934, 7.789], [-2.702, 9.287], [-8.068, 7.153], [4.645, 0.259], [5.149, 5.453], 
        [0.039, 4.844], [2.176, 3.908], [-7.825, 7.479], [-1.522, 10.145], [-2.856, 8.154], [4.108, 2.043], [4.998, 2.545], 
        [-7.353, -6.747], [-5.105, -6.595], [-7.706, -7.794], [2.568, 3.792], [3.333, 2.408], [-9.115, 8.142], [4.755, 2.996], 
        [-6.238, -7.654], [-10.887, 7.915], [-3.709, 9.919], [-9.605, 7.741], [-7.215, -6.676], [-1.234, 8.674],[-3.555, 8.045]] 

# 计算欧拉距离
def calcDis(dataSet, centroids, k):
    clalist = []
    for data in dataSet:
        diff = np.tile(data, (k, 1)) - centroids
        squaredDiff = diff ** 2
        squaredDist = np.sum(squaredDiff, axis=1)
        distance = squaredDist ** 0.5
        clalist.append(distance)

    clalist = np.array(clalist)
    return clalist

# 计算质心
def classify(dataSet, centroids, k):
    clalist = calcDis(dataSet, centroids, k)
    minDistIndices = np.argmin(clalist, axis=1)
    newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean()
    newCentroids = newCentroids.values
    changed = newCentroids - centroids
    return changed, newCentroids

# 使用k-means分类
def kmeans(dataSet, k):
    centroids = random.sample(dataSet, k)
    changed, newCentroids = classify(dataSet, centroids, k)
    while np.any(changed != 0):
        changed, newCentroids = classify(dataSet, newCentroids, k)

    centroids = sorted(newCentroids.tolist())
    cluster = []
    clalist = calcDis(dataSet, centroids, k)
    minDistIndices = np.argmin(clalist, axis=1)
    for i in range(k):
        cluster.append([])
    for i, j in enumerate(minDistIndices):
        cluster[j].append(dataSet[i])

    return centroids, cluster

# 创建数据集
def create_data_set(samples=300, features=2, clusters=3):
    X, _ = make_blobs(n_samples=samples, n_features=features, centers=clusters, random_state=42)
    data_set = X.tolist()
    return data_set

if __name__ == '__main__':
    n_samples = 300
    n_features = 2
    n_clusters = 5 

    # date = create_data_set(n_samples, n_features, n_clusters)
    # print(date)
    centroids, cluster = kmeans(date, n_clusters)

    # 绘制原始数据和聚类结果
    for i in range(len(date)):
        plt.scatter(date[i][0], date[i][1], marker='o', color='green', s=40, label='原始点')

    for j in range(len(centroids)):
        plt.scatter(centroids[j][0], centroids[j][1], marker='x', color='red', s=50, label='质心')

    # 按簇的颜色绘制
    for idx, points in enumerate(cluster):
        points = np.array(points)
        plt.scatter(points[:, 0], points[:, 1], marker='o', s=40, label=f'簇 {idx}')

    # plt.legend()
    plt.show()
